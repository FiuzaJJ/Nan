{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9PzhR41K8eJ-"
      },
      "outputs": [],
      "source": [
        "#import data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import importlib\n",
        "import glob\n",
        "import numpy as np \n",
        "\n",
        "#change paths\n",
        "arr = pd.read_csv(\"test/transfer_plate.csv\").iloc[:, 1:-4].to_numpy()\n",
        "transfer_plate_labels=pd.read_csv(\"test/transfer_plate.csv\").iloc[:, -4:].to_numpy()\n",
        "test_data = pd.read_csv(\"test/96_samples.csv\",header=None).iloc[:, 1:].to_numpy()\n",
        "\n",
        "csv_files = glob.glob(\"train/*.csv\")\n",
        "dataframes = {}\n",
        "for file in csv_files:\n",
        "\n",
        "    dataframes[file] = pd.read_csv(file)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "i05QVFKz-PUP"
      },
      "outputs": [],
      "source": [
        "transfer_plate_labels_1=[]\n",
        "for i in range(len(transfer_plate_labels)-1):\n",
        "\n",
        "    transfer_plate_labels_1.append(transfer_plate_labels[i])\n",
        "    transfer_plate_labels_1.append(transfer_plate_labels[i])\n",
        "\n",
        "transfer_plate_labels_1=np.array(transfer_plate_labels_1)[:192,-3:]\n",
        "transfer_plate_labels_1=np.array(transfer_plate_labels_1,dtype=np.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpF8WoNX_R6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9999999999999993"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from scipy.interpolate import interp1d\n",
        "\n",
        "target_wavenumbers = np.linspace(300, 1942, 1340)\n",
        "interpolated = []\n",
        "conc=[]\n",
        "\n",
        "for file in csv_files:\n",
        "    # Convert column names to float\n",
        "\n",
        "    wavelengths = dataframes[file].columns[:-5].astype(float)\n",
        "    # Get only the spectral data\n",
        "    intensities_y = (dataframes[file].values)\n",
        "    \n",
        "    conc.append(intensities_y[:,-5:-2])\n",
        "    #normalization steps --1-- scaling\n",
        "\n",
        "    intensities_y=intensities_y/(np.max(intensities_y))\n",
        "\n",
        "\n",
        "\n",
        "    # Interpolate each row (spectrum)\n",
        "\n",
        "    for row in intensities_y:\n",
        "\n",
        "        f = interp1d(wavelengths, row[:-5], kind='quadratic', fill_value='extrapolate')\n",
        "\n",
        "        interpolated.append(f(target_wavenumbers))\n",
        "\n",
        "\n",
        "\n",
        "interpolated=np.array(interpolated)\n",
        "\n",
        "#normalization step --2--\n",
        "\n",
        "interpolated=(interpolated-np.mean(interpolated))/np.std(interpolated)\n",
        "conc=np.concatenate(conc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-y0Oga8e_ZyM"
      },
      "outputs": [],
      "source": [
        "arr = np.char.strip(arr.astype(str), \"[]\").astype(float)\n",
        "test_data = np.char.strip(test_data.astype(str),\"[]\").astype(float)\n",
        "\n",
        "# normalization step --- 1 --- scaling\n",
        "arr=arr/np.max(arr)\n",
        "\n",
        "test_data=test_data/np.max(test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wK87nNGU_f61"
      },
      "outputs": [],
      "source": [
        "wavelengths=np.linspace(65,3350,2048)#make sure this is the actual interval.\n",
        "\n",
        "#interpolate test_data and the transfer_plate datasets(same format)\n",
        "transfer_plate=[]\n",
        "clean_test=[]\n",
        "for row in arr:\n",
        "\n",
        "    f = interp1d(wavelengths, row, kind='quadratic', fill_value='extrapolate')\n",
        "    transfer_plate.append(f(target_wavenumbers))\n",
        "\n",
        "for row in test_data:\n",
        "\n",
        "    g = interp1d(wavelengths, row, kind='quadratic', fill_value='extrapolate')\n",
        "\n",
        "    clean_test.append(g(target_wavenumbers))\n",
        "\n",
        "\n",
        "transfer_plate=(transfer_plate-np.mean(transfer_plate))/np.std(transfer_plate)\n",
        "clean_test=(clean_test-np.mean(clean_test))/np.std(clean_test)\n",
        "\n",
        "\n",
        "transfer_plate=np.array(transfer_plate,dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URa6QNJG_hIx",
        "outputId": "c09126f1-c4fb-40c4-b7c9-199f76601a61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([16.7344 ,  3.05065,  3.94541])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import Data_augment as Da\n",
        "importlib.reload(Da)\n",
        "\n",
        "train_data=np.concatenate((transfer_plate,interpolated))\n",
        "labels=np.concatenate((transfer_plate_labels_1,conc))\n",
        "labels_scaled,labels_max= Da.reversible_scaler(labels,None)\n",
        "\n",
        "labels_max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgFHLmYb_8oy",
        "outputId": "337e8d60-69b9-4571-f784-9b43f5701d73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "torch.Size([32, 1, 1340])\n",
            "torch.Size([32, 3])\n"
          ]
        }
      ],
      "source": [
        "from Raman_Dataset import RamanDataset\n",
        "from torch.utils.data import DataLoader\n",
        "import Data_augment as Da\n",
        "importlib.reload(Da)\n",
        "\n",
        "\n",
        "\n",
        "dataset1 = RamanDataset(train_data, labels_scaled)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "loader = DataLoader(dataset1, batch_size=32, shuffle=True)\n",
        "\n",
        "# Example: iterate\n",
        "for spectra_batch, targets_batch in loader:\n",
        "    print(spectra_batch.shape)  # (batch_size, 1, num_points)\n",
        "\n",
        "    print(targets_batch.shape)  # (batch_size, 3)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZpzGVZXArOG"
      },
      "source": [
        "-------Big Model training ------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtIm7mtZAAcu"
      },
      "outputs": [],
      "source": [
        "import SpectraCNN as CNN\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "importlib.reload(CNN)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "model = CNN.SpectraCNN()\n",
        "\n",
        "model = model.to(device) #move to gpu if available\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "epochs=100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    running_loss=0\n",
        "    print(epoch)\n",
        "    for X_batch, y_batch in loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(X_batch)\n",
        "        loss = criterion(preds, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch}: Loss {running_loss/len(loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "c8xvW0BoAiUZ"
      },
      "outputs": [],
      "source": [
        "clean_test=np.array(clean_test,dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "IQ56fNeiAl3-"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[29], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(RamanDataset(clean_test,\u001b[38;5;28;01mNone\u001b[39;00m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# switch to evaluation mode\u001b[39;00m\n\u001b[0;32m      5\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
            "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "test_loader = DataLoader(RamanDataset(clean_test,None, train=False), batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "model.eval()  # switch to evaluation mode\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for spectra in test_loader:\n",
        "        outputs = model(spectra)  # shape (batch, 3)\n",
        "        predictions.append(outputs)\n",
        "\n",
        "predictions = torch.cat(predictions).numpy()\n",
        "predictions2=Da.reversible_scaler(predictions,labels_max,reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tyy2d_QBBCGC"
      },
      "source": [
        "Cross validation for model tuning and ensembling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "qG-MvZLUA264",
        "outputId": "b9330984-1ba8-4b54-db7b-92946897d077"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---- Fold 1/5 ----\n",
            "Epoch 1/250 - Val Loss: 0.0644\n",
            "Epoch 2/250 - Val Loss: 0.0587\n",
            "Epoch 3/250 - Val Loss: 0.0483\n",
            "Epoch 4/250 - Val Loss: 0.0446\n",
            "Epoch 5/250 - Val Loss: 0.0476\n",
            "Epoch 6/250 - Val Loss: 0.0424\n",
            "Epoch 7/250 - Val Loss: 0.0411\n",
            "Epoch 8/250 - Val Loss: 0.0415\n",
            "Epoch 9/250 - Val Loss: 0.0409\n",
            "Epoch 10/250 - Val Loss: 0.0408\n",
            "Epoch 11/250 - Val Loss: 0.0385\n",
            "Epoch 12/250 - Val Loss: 0.0386\n",
            "Epoch 13/250 - Val Loss: 0.0390\n",
            "Epoch 14/250 - Val Loss: 0.0382\n",
            "Epoch 15/250 - Val Loss: 0.0374\n",
            "Epoch 16/250 - Val Loss: 0.0364\n",
            "Epoch 17/250 - Val Loss: 0.0362\n",
            "Epoch 18/250 - Val Loss: 0.0367\n",
            "Epoch 19/250 - Val Loss: 0.0369\n",
            "Epoch 20/250 - Val Loss: 0.0385\n",
            "Epoch 21/250 - Val Loss: 0.0352\n",
            "Epoch 22/250 - Val Loss: 0.0359\n",
            "Epoch 23/250 - Val Loss: 0.0346\n",
            "Epoch 24/250 - Val Loss: 0.0339\n",
            "Epoch 25/250 - Val Loss: 0.0346\n",
            "Epoch 26/250 - Val Loss: 0.0330\n",
            "Epoch 27/250 - Val Loss: 0.0317\n",
            "Epoch 28/250 - Val Loss: 0.0348\n",
            "Epoch 29/250 - Val Loss: 0.0353\n",
            "Epoch 30/250 - Val Loss: 0.0355\n",
            "Epoch 31/250 - Val Loss: 0.0331\n",
            "Epoch 32/250 - Val Loss: 0.0397\n",
            "Epoch 33/250 - Val Loss: 0.0309\n",
            "Epoch 34/250 - Val Loss: 0.0325\n",
            "Epoch 35/250 - Val Loss: 0.0370\n",
            "Epoch 36/250 - Val Loss: 0.0297\n",
            "Epoch 37/250 - Val Loss: 0.0328\n",
            "Epoch 38/250 - Val Loss: 0.0310\n",
            "Epoch 39/250 - Val Loss: 0.0322\n",
            "Epoch 40/250 - Val Loss: 0.0306\n",
            "Epoch 41/250 - Val Loss: 0.0312\n",
            "Epoch 42/250 - Val Loss: 0.0291\n",
            "Epoch 43/250 - Val Loss: 0.0292\n",
            "Epoch 44/250 - Val Loss: 0.0285\n",
            "Epoch 45/250 - Val Loss: 0.0299\n",
            "Epoch 46/250 - Val Loss: 0.0341\n",
            "Epoch 47/250 - Val Loss: 0.0292\n",
            "Epoch 48/250 - Val Loss: 0.0332\n",
            "Epoch 49/250 - Val Loss: 0.0316\n",
            "Epoch 50/250 - Val Loss: 0.0309\n",
            "Epoch 51/250 - Val Loss: 0.0272\n",
            "Epoch 52/250 - Val Loss: 0.0320\n",
            "Epoch 53/250 - Val Loss: 0.0256\n",
            "Epoch 54/250 - Val Loss: 0.0277\n",
            "Epoch 55/250 - Val Loss: 0.0251\n",
            "Epoch 56/250 - Val Loss: 0.0255\n",
            "Epoch 57/250 - Val Loss: 0.0260\n",
            "Epoch 58/250 - Val Loss: 0.0331\n",
            "Epoch 59/250 - Val Loss: 0.0245\n",
            "Epoch 60/250 - Val Loss: 0.0255\n",
            "Epoch 61/250 - Val Loss: 0.0311\n",
            "Epoch 62/250 - Val Loss: 0.0261\n",
            "Epoch 63/250 - Val Loss: 0.0243\n",
            "Epoch 64/250 - Val Loss: 0.0274\n",
            "Epoch 65/250 - Val Loss: 0.0309\n",
            "Epoch 66/250 - Val Loss: 0.0337\n",
            "Epoch 67/250 - Val Loss: 0.0266\n",
            "Epoch 68/250 - Val Loss: 0.0241\n",
            "Epoch 69/250 - Val Loss: 0.0229\n",
            "Epoch 70/250 - Val Loss: 0.0279\n",
            "Epoch 71/250 - Val Loss: 0.0243\n",
            "Epoch 72/250 - Val Loss: 0.0227\n",
            "Epoch 73/250 - Val Loss: 0.0241\n",
            "Epoch 74/250 - Val Loss: 0.0286\n",
            "Epoch 75/250 - Val Loss: 0.0219\n",
            "Epoch 76/250 - Val Loss: 0.0219\n",
            "Epoch 77/250 - Val Loss: 0.0233\n",
            "Epoch 78/250 - Val Loss: 0.0220\n",
            "Epoch 79/250 - Val Loss: 0.0220\n",
            "Epoch 80/250 - Val Loss: 0.0202\n",
            "Epoch 81/250 - Val Loss: 0.0196\n",
            "Epoch 82/250 - Val Loss: 0.0211\n",
            "Epoch 83/250 - Val Loss: 0.0209\n",
            "Epoch 84/250 - Val Loss: 0.0217\n",
            "Epoch 85/250 - Val Loss: 0.0243\n",
            "Epoch 86/250 - Val Loss: 0.0215\n",
            "Epoch 87/250 - Val Loss: 0.0229\n",
            "Epoch 88/250 - Val Loss: 0.0227\n",
            "Epoch 89/250 - Val Loss: 0.0191\n",
            "Epoch 90/250 - Val Loss: 0.0188\n",
            "Epoch 91/250 - Val Loss: 0.0300\n",
            "Epoch 92/250 - Val Loss: 0.0227\n",
            "Epoch 93/250 - Val Loss: 0.0208\n",
            "Epoch 94/250 - Val Loss: 0.0226\n",
            "Epoch 95/250 - Val Loss: 0.0198\n",
            "Epoch 96/250 - Val Loss: 0.0303\n",
            "Epoch 97/250 - Val Loss: 0.0229\n",
            "Epoch 98/250 - Val Loss: 0.0193\n",
            "Epoch 99/250 - Val Loss: 0.0205\n",
            "Epoch 100/250 - Val Loss: 0.0222\n",
            "Epoch 101/250 - Val Loss: 0.0162\n",
            "Epoch 102/250 - Val Loss: 0.0205\n",
            "Epoch 103/250 - Val Loss: 0.0172\n",
            "Epoch 104/250 - Val Loss: 0.0194\n",
            "Epoch 105/250 - Val Loss: 0.0177\n",
            "Epoch 106/250 - Val Loss: 0.0233\n",
            "Epoch 107/250 - Val Loss: 0.0157\n",
            "Epoch 108/250 - Val Loss: 0.0137\n",
            "Epoch 109/250 - Val Loss: 0.0145\n",
            "Epoch 110/250 - Val Loss: 0.0236\n",
            "Epoch 111/250 - Val Loss: 0.0275\n",
            "Epoch 112/250 - Val Loss: 0.0129\n",
            "Epoch 113/250 - Val Loss: 0.0147\n",
            "Epoch 114/250 - Val Loss: 0.0137\n",
            "Epoch 115/250 - Val Loss: 0.0197\n",
            "Epoch 116/250 - Val Loss: 0.0207\n",
            "Epoch 117/250 - Val Loss: 0.0189\n",
            "Epoch 118/250 - Val Loss: 0.0171\n",
            "Epoch 119/250 - Val Loss: 0.0211\n",
            "Epoch 120/250 - Val Loss: 0.0158\n",
            "Epoch 121/250 - Val Loss: 0.0116\n",
            "Epoch 122/250 - Val Loss: 0.0188\n",
            "Epoch 123/250 - Val Loss: 0.0119\n",
            "Epoch 124/250 - Val Loss: 0.0151\n",
            "Epoch 125/250 - Val Loss: 0.0153\n",
            "Epoch 126/250 - Val Loss: 0.0157\n",
            "Epoch 127/250 - Val Loss: 0.0183\n",
            "Epoch 128/250 - Val Loss: 0.0131\n",
            "Epoch 129/250 - Val Loss: 0.0108\n",
            "Epoch 130/250 - Val Loss: 0.0123\n",
            "Epoch 131/250 - Val Loss: 0.0144\n",
            "Epoch 132/250 - Val Loss: 0.0134\n",
            "Epoch 133/250 - Val Loss: 0.0123\n",
            "Epoch 134/250 - Val Loss: 0.0170\n",
            "Epoch 135/250 - Val Loss: 0.0105\n",
            "Epoch 136/250 - Val Loss: 0.0106\n",
            "Epoch 137/250 - Val Loss: 0.0143\n",
            "Epoch 138/250 - Val Loss: 0.0120\n",
            "Epoch 139/250 - Val Loss: 0.0104\n",
            "Epoch 140/250 - Val Loss: 0.0116\n",
            "Epoch 141/250 - Val Loss: 0.0131\n",
            "Epoch 142/250 - Val Loss: 0.0130\n",
            "Epoch 143/250 - Val Loss: 0.0124\n",
            "Epoch 144/250 - Val Loss: 0.0273\n",
            "Epoch 145/250 - Val Loss: 0.0120\n",
            "Epoch 146/250 - Val Loss: 0.0125\n",
            "Epoch 147/250 - Val Loss: 0.0172\n",
            "Epoch 148/250 - Val Loss: 0.0149\n",
            "Epoch 149/250 - Val Loss: 0.0150\n",
            "Epoch 150/250 - Val Loss: 0.0134\n",
            "Epoch 151/250 - Val Loss: 0.0129\n",
            "Epoch 152/250 - Val Loss: 0.0121\n",
            "Epoch 153/250 - Val Loss: 0.0110\n",
            "Epoch 154/250 - Val Loss: 0.0117\n",
            "Epoch 155/250 - Val Loss: 0.0131\n",
            "Epoch 156/250 - Val Loss: 0.0143\n",
            "Epoch 157/250 - Val Loss: 0.0114\n",
            "Epoch 158/250 - Val Loss: 0.0212\n",
            "Epoch 159/250 - Val Loss: 0.0114\n",
            "Epoch 160/250 - Val Loss: 0.0149\n",
            "Epoch 161/250 - Val Loss: 0.0128\n",
            "Epoch 162/250 - Val Loss: 0.0121\n",
            "Epoch 163/250 - Val Loss: 0.0125\n",
            "Epoch 164/250 - Val Loss: 0.0150\n",
            "Epoch 165/250 - Val Loss: 0.0140\n",
            "Epoch 166/250 - Val Loss: 0.0124\n",
            "Epoch 167/250 - Val Loss: 0.0117\n",
            "Epoch 168/250 - Val Loss: 0.0101\n",
            "Epoch 169/250 - Val Loss: 0.0117\n",
            "Epoch 170/250 - Val Loss: 0.0110\n",
            "Epoch 171/250 - Val Loss: 0.0109\n",
            "Epoch 172/250 - Val Loss: 0.0099\n",
            "Epoch 173/250 - Val Loss: 0.0110\n",
            "Epoch 174/250 - Val Loss: 0.0094\n",
            "Epoch 175/250 - Val Loss: 0.0202\n",
            "Epoch 176/250 - Val Loss: 0.0109\n",
            "Epoch 177/250 - Val Loss: 0.0130\n",
            "Epoch 178/250 - Val Loss: 0.0141\n",
            "Epoch 179/250 - Val Loss: 0.0099\n",
            "Epoch 180/250 - Val Loss: 0.0102\n",
            "Epoch 181/250 - Val Loss: 0.0108\n",
            "Epoch 182/250 - Val Loss: 0.0109\n",
            "Epoch 183/250 - Val Loss: 0.0098\n",
            "Epoch 184/250 - Val Loss: 0.0116\n",
            "Epoch 185/250 - Val Loss: 0.0143\n",
            "Epoch 186/250 - Val Loss: 0.0122\n",
            "Epoch 187/250 - Val Loss: 0.0092\n",
            "Epoch 188/250 - Val Loss: 0.0138\n",
            "Epoch 189/250 - Val Loss: 0.0107\n",
            "Epoch 190/250 - Val Loss: 0.0147\n",
            "Epoch 191/250 - Val Loss: 0.0094\n",
            "Epoch 192/250 - Val Loss: 0.0091\n",
            "Epoch 193/250 - Val Loss: 0.0110\n",
            "Epoch 194/250 - Val Loss: 0.0167\n",
            "Epoch 195/250 - Val Loss: 0.0135\n",
            "Epoch 196/250 - Val Loss: 0.0105\n",
            "Epoch 197/250 - Val Loss: 0.0138\n",
            "Epoch 198/250 - Val Loss: 0.0199\n",
            "Epoch 199/250 - Val Loss: 0.0135\n",
            "Epoch 200/250 - Val Loss: 0.0126\n",
            "Epoch 201/250 - Val Loss: 0.0092\n",
            "Epoch 202/250 - Val Loss: 0.0093\n",
            "Epoch 203/250 - Val Loss: 0.0159\n",
            "Epoch 204/250 - Val Loss: 0.0135\n",
            "Epoch 205/250 - Val Loss: 0.0115\n",
            "Epoch 206/250 - Val Loss: 0.0102\n",
            "Epoch 207/250 - Val Loss: 0.0107\n",
            "Epoch 208/250 - Val Loss: 0.0097\n",
            "Epoch 209/250 - Val Loss: 0.0093\n",
            "Epoch 210/250 - Val Loss: 0.0092\n",
            "Epoch 211/250 - Val Loss: 0.0113\n",
            "Epoch 212/250 - Val Loss: 0.0120\n",
            "Epoch 213/250 - Val Loss: 0.0124\n",
            "Epoch 214/250 - Val Loss: 0.0103\n",
            "Epoch 215/250 - Val Loss: 0.0102\n",
            "Epoch 216/250 - Val Loss: 0.0100\n",
            "Epoch 217/250 - Val Loss: 0.0092\n",
            "Epoch 218/250 - Val Loss: 0.0097\n",
            "Epoch 219/250 - Val Loss: 0.0113\n",
            "Epoch 220/250 - Val Loss: 0.0089\n",
            "Epoch 221/250 - Val Loss: 0.0085\n",
            "Epoch 222/250 - Val Loss: 0.0110\n",
            "Epoch 223/250 - Val Loss: 0.0098\n",
            "Epoch 224/250 - Val Loss: 0.0092\n",
            "Epoch 225/250 - Val Loss: 0.0114\n",
            "Epoch 226/250 - Val Loss: 0.0089\n",
            "Epoch 227/250 - Val Loss: 0.0080\n",
            "Epoch 228/250 - Val Loss: 0.0110\n",
            "Epoch 229/250 - Val Loss: 0.0091\n",
            "Epoch 230/250 - Val Loss: 0.0101\n",
            "Epoch 231/250 - Val Loss: 0.0103\n",
            "Epoch 232/250 - Val Loss: 0.0093\n",
            "Epoch 233/250 - Val Loss: 0.0090\n",
            "Epoch 234/250 - Val Loss: 0.0102\n",
            "Epoch 235/250 - Val Loss: 0.0093\n",
            "Epoch 236/250 - Val Loss: 0.0102\n",
            "Epoch 237/250 - Val Loss: 0.0083\n",
            "Epoch 238/250 - Val Loss: 0.0087\n",
            "Epoch 239/250 - Val Loss: 0.0113\n",
            "Epoch 240/250 - Val Loss: 0.0088\n",
            "Epoch 241/250 - Val Loss: 0.0127\n",
            "Epoch 242/250 - Val Loss: 0.0126\n",
            "Epoch 243/250 - Val Loss: 0.0096\n",
            "Epoch 244/250 - Val Loss: 0.0119\n",
            "Epoch 245/250 - Val Loss: 0.0129\n",
            "Epoch 246/250 - Val Loss: 0.0085\n",
            "Epoch 247/250 - Val Loss: 0.0088\n",
            "Epoch 248/250 - Val Loss: 0.0088\n",
            "Epoch 249/250 - Val Loss: 0.0114\n",
            "Epoch 250/250 - Val Loss: 0.0105\n",
            "\n",
            "---- Fold 2/5 ----\n",
            "Epoch 1/250 - Val Loss: 0.0723\n",
            "Epoch 2/250 - Val Loss: 0.0625\n",
            "Epoch 3/250 - Val Loss: 0.0503\n",
            "Epoch 4/250 - Val Loss: 0.0463\n",
            "Epoch 5/250 - Val Loss: 0.0435\n",
            "Epoch 6/250 - Val Loss: 0.0436\n",
            "Epoch 7/250 - Val Loss: 0.0436\n",
            "Epoch 8/250 - Val Loss: 0.0534\n",
            "Epoch 9/250 - Val Loss: 0.0409\n",
            "Epoch 10/250 - Val Loss: 0.0412\n",
            "Epoch 11/250 - Val Loss: 0.0433\n",
            "Epoch 12/250 - Val Loss: 0.0417\n",
            "Epoch 13/250 - Val Loss: 0.0402\n",
            "Epoch 14/250 - Val Loss: 0.0396\n",
            "Epoch 15/250 - Val Loss: 0.0387\n",
            "Epoch 16/250 - Val Loss: 0.0519\n",
            "Epoch 17/250 - Val Loss: 0.0387\n",
            "Epoch 18/250 - Val Loss: 0.0367\n",
            "Epoch 19/250 - Val Loss: 0.0384\n",
            "Epoch 20/250 - Val Loss: 0.0366\n",
            "Epoch 21/250 - Val Loss: 0.0410\n",
            "Epoch 22/250 - Val Loss: 0.0361\n",
            "Epoch 23/250 - Val Loss: 0.0345\n",
            "Epoch 24/250 - Val Loss: 0.0373\n",
            "Epoch 25/250 - Val Loss: 0.0369\n",
            "Epoch 26/250 - Val Loss: 0.0386\n",
            "Epoch 27/250 - Val Loss: 0.0348\n",
            "Epoch 28/250 - Val Loss: 0.0424\n",
            "Epoch 29/250 - Val Loss: 0.0335\n",
            "Epoch 30/250 - Val Loss: 0.0350\n",
            "Epoch 31/250 - Val Loss: 0.0333\n",
            "Epoch 32/250 - Val Loss: 0.0456\n",
            "Epoch 33/250 - Val Loss: 0.0363\n",
            "Epoch 34/250 - Val Loss: 0.0353\n",
            "Epoch 35/250 - Val Loss: 0.0314\n",
            "Epoch 36/250 - Val Loss: 0.0359\n",
            "Epoch 37/250 - Val Loss: 0.0315\n",
            "Epoch 38/250 - Val Loss: 0.0324\n",
            "Epoch 39/250 - Val Loss: 0.0360\n",
            "Epoch 40/250 - Val Loss: 0.0305\n",
            "Epoch 41/250 - Val Loss: 0.0323\n",
            "Epoch 42/250 - Val Loss: 0.0315\n",
            "Epoch 43/250 - Val Loss: 0.0318\n",
            "Epoch 44/250 - Val Loss: 0.0300\n",
            "Epoch 45/250 - Val Loss: 0.0317\n",
            "Epoch 46/250 - Val Loss: 0.0291\n",
            "Epoch 47/250 - Val Loss: 0.0318\n",
            "Epoch 48/250 - Val Loss: 0.0317\n",
            "Epoch 49/250 - Val Loss: 0.0319\n",
            "Epoch 50/250 - Val Loss: 0.0288\n",
            "Epoch 51/250 - Val Loss: 0.0296\n",
            "Epoch 52/250 - Val Loss: 0.0290\n",
            "Epoch 53/250 - Val Loss: 0.0497\n",
            "Epoch 54/250 - Val Loss: 0.0274\n",
            "Epoch 55/250 - Val Loss: 0.0299\n",
            "Epoch 56/250 - Val Loss: 0.0291\n",
            "Epoch 57/250 - Val Loss: 0.0279\n",
            "Epoch 58/250 - Val Loss: 0.0262\n",
            "Epoch 59/250 - Val Loss: 0.0287\n",
            "Epoch 60/250 - Val Loss: 0.0305\n",
            "Epoch 61/250 - Val Loss: 0.0381\n",
            "Epoch 62/250 - Val Loss: 0.0278\n",
            "Epoch 63/250 - Val Loss: 0.0258\n",
            "Epoch 64/250 - Val Loss: 0.0410\n",
            "Epoch 65/250 - Val Loss: 0.0250\n",
            "Epoch 66/250 - Val Loss: 0.0302\n",
            "Epoch 67/250 - Val Loss: 0.0250\n",
            "Epoch 68/250 - Val Loss: 0.0286\n",
            "Epoch 69/250 - Val Loss: 0.0247\n",
            "Epoch 70/250 - Val Loss: 0.0283\n",
            "Epoch 71/250 - Val Loss: 0.0245\n",
            "Epoch 72/250 - Val Loss: 0.0242\n",
            "Epoch 73/250 - Val Loss: 0.0237\n",
            "Epoch 74/250 - Val Loss: 0.0286\n",
            "Epoch 75/250 - Val Loss: 0.0302\n",
            "Epoch 76/250 - Val Loss: 0.0263\n",
            "Epoch 77/250 - Val Loss: 0.0237\n",
            "Epoch 78/250 - Val Loss: 0.0248\n",
            "Epoch 79/250 - Val Loss: 0.0258\n",
            "Epoch 80/250 - Val Loss: 0.0202\n",
            "Epoch 81/250 - Val Loss: 0.0249\n",
            "Epoch 82/250 - Val Loss: 0.0320\n",
            "Epoch 83/250 - Val Loss: 0.0267\n",
            "Epoch 84/250 - Val Loss: 0.0244\n",
            "Epoch 85/250 - Val Loss: 0.0218\n",
            "Epoch 86/250 - Val Loss: 0.0189\n",
            "Epoch 87/250 - Val Loss: 0.0205\n",
            "Epoch 88/250 - Val Loss: 0.0194\n",
            "Epoch 89/250 - Val Loss: 0.0279\n",
            "Epoch 90/250 - Val Loss: 0.0238\n",
            "Epoch 91/250 - Val Loss: 0.0198\n",
            "Epoch 92/250 - Val Loss: 0.0205\n",
            "Epoch 93/250 - Val Loss: 0.0206\n",
            "Epoch 94/250 - Val Loss: 0.0292\n",
            "Epoch 95/250 - Val Loss: 0.0157\n",
            "Epoch 96/250 - Val Loss: 0.0178\n",
            "Epoch 97/250 - Val Loss: 0.0173\n",
            "Epoch 98/250 - Val Loss: 0.0199\n",
            "Epoch 99/250 - Val Loss: 0.0155\n",
            "Epoch 100/250 - Val Loss: 0.0159\n",
            "Epoch 101/250 - Val Loss: 0.0243\n",
            "Epoch 102/250 - Val Loss: 0.0164\n",
            "Epoch 103/250 - Val Loss: 0.0138\n",
            "Epoch 104/250 - Val Loss: 0.0169\n",
            "Epoch 105/250 - Val Loss: 0.0154\n",
            "Epoch 106/250 - Val Loss: 0.0176\n",
            "Epoch 107/250 - Val Loss: 0.0156\n",
            "Epoch 108/250 - Val Loss: 0.0152\n",
            "Epoch 109/250 - Val Loss: 0.0172\n",
            "Epoch 110/250 - Val Loss: 0.0169\n",
            "Epoch 111/250 - Val Loss: 0.0220\n",
            "Epoch 112/250 - Val Loss: 0.0202\n",
            "Epoch 113/250 - Val Loss: 0.0144\n",
            "Epoch 114/250 - Val Loss: 0.0143\n",
            "Epoch 115/250 - Val Loss: 0.0151\n",
            "Epoch 116/250 - Val Loss: 0.0245\n",
            "Epoch 117/250 - Val Loss: 0.0162\n",
            "Epoch 118/250 - Val Loss: 0.0160\n",
            "Epoch 119/250 - Val Loss: 0.0134\n",
            "Epoch 120/250 - Val Loss: 0.0143\n",
            "Epoch 121/250 - Val Loss: 0.0129\n",
            "Epoch 122/250 - Val Loss: 0.0192\n",
            "Epoch 123/250 - Val Loss: 0.0130\n",
            "Epoch 124/250 - Val Loss: 0.0120\n",
            "Epoch 125/250 - Val Loss: 0.0162\n",
            "Epoch 126/250 - Val Loss: 0.0156\n",
            "Epoch 127/250 - Val Loss: 0.0152\n",
            "Epoch 128/250 - Val Loss: 0.0181\n",
            "Epoch 129/250 - Val Loss: 0.0123\n",
            "Epoch 130/250 - Val Loss: 0.0128\n",
            "Epoch 131/250 - Val Loss: 0.0155\n",
            "Epoch 132/250 - Val Loss: 0.0142\n",
            "Epoch 133/250 - Val Loss: 0.0130\n",
            "Epoch 134/250 - Val Loss: 0.0128\n",
            "Epoch 135/250 - Val Loss: 0.0130\n",
            "Epoch 136/250 - Val Loss: 0.0117\n",
            "Epoch 137/250 - Val Loss: 0.0127\n",
            "Epoch 138/250 - Val Loss: 0.0135\n",
            "Epoch 139/250 - Val Loss: 0.0110\n",
            "Epoch 140/250 - Val Loss: 0.0109\n",
            "Epoch 141/250 - Val Loss: 0.0133\n",
            "Epoch 142/250 - Val Loss: 0.0114\n",
            "Epoch 143/250 - Val Loss: 0.0135\n",
            "Epoch 144/250 - Val Loss: 0.0130\n",
            "Epoch 145/250 - Val Loss: 0.0119\n",
            "Epoch 146/250 - Val Loss: 0.0134\n",
            "Epoch 147/250 - Val Loss: 0.0105\n",
            "Epoch 148/250 - Val Loss: 0.0124\n",
            "Epoch 149/250 - Val Loss: 0.0113\n",
            "Epoch 150/250 - Val Loss: 0.0135\n",
            "Epoch 151/250 - Val Loss: 0.0130\n",
            "Epoch 152/250 - Val Loss: 0.0117\n",
            "Epoch 153/250 - Val Loss: 0.0130\n",
            "Epoch 154/250 - Val Loss: 0.0170\n",
            "Epoch 155/250 - Val Loss: 0.0131\n",
            "Epoch 156/250 - Val Loss: 0.0110\n",
            "Epoch 157/250 - Val Loss: 0.0110\n",
            "Epoch 158/250 - Val Loss: 0.0100\n",
            "Epoch 159/250 - Val Loss: 0.0096\n",
            "Epoch 160/250 - Val Loss: 0.0100\n",
            "Epoch 161/250 - Val Loss: 0.0131\n",
            "Epoch 162/250 - Val Loss: 0.0110\n",
            "Epoch 163/250 - Val Loss: 0.0098\n",
            "Epoch 164/250 - Val Loss: 0.0114\n",
            "Epoch 165/250 - Val Loss: 0.0112\n",
            "Epoch 166/250 - Val Loss: 0.0111\n",
            "Epoch 167/250 - Val Loss: 0.0111\n",
            "Epoch 168/250 - Val Loss: 0.0142\n",
            "Epoch 169/250 - Val Loss: 0.0144\n",
            "Epoch 170/250 - Val Loss: 0.0096\n",
            "Epoch 171/250 - Val Loss: 0.0108\n",
            "Epoch 172/250 - Val Loss: 0.0104\n",
            "Epoch 173/250 - Val Loss: 0.0102\n",
            "Epoch 174/250 - Val Loss: 0.0116\n",
            "Epoch 175/250 - Val Loss: 0.0096\n",
            "Epoch 176/250 - Val Loss: 0.0157\n",
            "Epoch 177/250 - Val Loss: 0.0129\n",
            "Epoch 178/250 - Val Loss: 0.0174\n",
            "Epoch 179/250 - Val Loss: 0.0135\n",
            "Epoch 180/250 - Val Loss: 0.0110\n",
            "Epoch 181/250 - Val Loss: 0.0101\n",
            "Epoch 182/250 - Val Loss: 0.0117\n",
            "Epoch 183/250 - Val Loss: 0.0128\n",
            "Epoch 184/250 - Val Loss: 0.0116\n",
            "Epoch 185/250 - Val Loss: 0.0120\n",
            "Epoch 186/250 - Val Loss: 0.0095\n",
            "Epoch 187/250 - Val Loss: 0.0099\n",
            "Epoch 188/250 - Val Loss: 0.0098\n",
            "Epoch 189/250 - Val Loss: 0.0116\n",
            "Epoch 190/250 - Val Loss: 0.0104\n",
            "Epoch 191/250 - Val Loss: 0.0132\n",
            "Epoch 192/250 - Val Loss: 0.0098\n",
            "Epoch 193/250 - Val Loss: 0.0097\n",
            "Epoch 194/250 - Val Loss: 0.0106\n",
            "Epoch 195/250 - Val Loss: 0.0100\n",
            "Epoch 196/250 - Val Loss: 0.0124\n",
            "Epoch 197/250 - Val Loss: 0.0114\n",
            "Epoch 198/250 - Val Loss: 0.0112\n",
            "Epoch 199/250 - Val Loss: 0.0141\n",
            "Epoch 200/250 - Val Loss: 0.0101\n",
            "Epoch 201/250 - Val Loss: 0.0107\n",
            "Epoch 202/250 - Val Loss: 0.0097\n",
            "Epoch 203/250 - Val Loss: 0.0142\n",
            "Epoch 204/250 - Val Loss: 0.0093\n",
            "Epoch 205/250 - Val Loss: 0.0094\n",
            "Epoch 206/250 - Val Loss: 0.0090\n",
            "Epoch 207/250 - Val Loss: 0.0100\n",
            "Epoch 208/250 - Val Loss: 0.0088\n",
            "Epoch 209/250 - Val Loss: 0.0102\n",
            "Epoch 210/250 - Val Loss: 0.0106\n",
            "Epoch 211/250 - Val Loss: 0.0096\n",
            "Epoch 212/250 - Val Loss: 0.0132\n",
            "Epoch 213/250 - Val Loss: 0.0095\n",
            "Epoch 214/250 - Val Loss: 0.0109\n",
            "Epoch 215/250 - Val Loss: 0.0101\n",
            "Epoch 216/250 - Val Loss: 0.0096\n",
            "Epoch 217/250 - Val Loss: 0.0100\n",
            "Epoch 218/250 - Val Loss: 0.0107\n",
            "Epoch 219/250 - Val Loss: 0.0114\n",
            "Epoch 220/250 - Val Loss: 0.0092\n",
            "Epoch 221/250 - Val Loss: 0.0112\n",
            "Epoch 222/250 - Val Loss: 0.0149\n",
            "Epoch 223/250 - Val Loss: 0.0100\n",
            "Epoch 224/250 - Val Loss: 0.0143\n",
            "Epoch 225/250 - Val Loss: 0.0150\n",
            "Epoch 226/250 - Val Loss: 0.0093\n",
            "Epoch 227/250 - Val Loss: 0.0106\n",
            "Epoch 228/250 - Val Loss: 0.0094\n",
            "Epoch 229/250 - Val Loss: 0.0094\n",
            "Epoch 230/250 - Val Loss: 0.0094\n",
            "Epoch 231/250 - Val Loss: 0.0099\n",
            "Epoch 232/250 - Val Loss: 0.0096\n",
            "Epoch 233/250 - Val Loss: 0.0101\n",
            "Epoch 234/250 - Val Loss: 0.0096\n",
            "Epoch 235/250 - Val Loss: 0.0097\n",
            "Epoch 236/250 - Val Loss: 0.0096\n",
            "Epoch 237/250 - Val Loss: 0.0101\n",
            "Epoch 238/250 - Val Loss: 0.0093\n",
            "Epoch 239/250 - Val Loss: 0.0093\n",
            "Epoch 240/250 - Val Loss: 0.0112\n",
            "Epoch 241/250 - Val Loss: 0.0083\n",
            "Epoch 242/250 - Val Loss: 0.0110\n",
            "Epoch 243/250 - Val Loss: 0.0091\n",
            "Epoch 244/250 - Val Loss: 0.0120\n",
            "Epoch 245/250 - Val Loss: 0.0122\n",
            "Epoch 246/250 - Val Loss: 0.0101\n",
            "Epoch 247/250 - Val Loss: 0.0093\n",
            "Epoch 248/250 - Val Loss: 0.0126\n",
            "Epoch 249/250 - Val Loss: 0.0090\n",
            "Epoch 250/250 - Val Loss: 0.0105\n",
            "\n",
            "---- Fold 3/5 ----\n",
            "Epoch 1/250 - Val Loss: 0.0692\n",
            "Epoch 2/250 - Val Loss: 0.0652\n",
            "Epoch 3/250 - Val Loss: 0.0590\n",
            "Epoch 4/250 - Val Loss: 0.0523\n",
            "Epoch 5/250 - Val Loss: 0.0445\n",
            "Epoch 6/250 - Val Loss: 0.0447\n",
            "Epoch 7/250 - Val Loss: 0.0431\n",
            "Epoch 8/250 - Val Loss: 0.0580\n",
            "Epoch 9/250 - Val Loss: 0.0598\n",
            "Epoch 10/250 - Val Loss: 0.0413\n",
            "Epoch 11/250 - Val Loss: 0.0412\n",
            "Epoch 12/250 - Val Loss: 0.0413\n",
            "Epoch 13/250 - Val Loss: 0.0492\n",
            "Epoch 14/250 - Val Loss: 0.0724\n",
            "Epoch 15/250 - Val Loss: 0.0380\n",
            "Epoch 16/250 - Val Loss: 0.0374\n",
            "Epoch 17/250 - Val Loss: 0.0367\n",
            "Epoch 18/250 - Val Loss: 0.0363\n",
            "Epoch 19/250 - Val Loss: 0.0360\n",
            "Epoch 20/250 - Val Loss: 0.0398\n",
            "Epoch 21/250 - Val Loss: 0.0430\n",
            "Epoch 22/250 - Val Loss: 0.0345\n",
            "Epoch 23/250 - Val Loss: 0.0372\n",
            "Epoch 24/250 - Val Loss: 0.0377\n",
            "Epoch 25/250 - Val Loss: 0.0358\n",
            "Epoch 26/250 - Val Loss: 0.0368\n",
            "Epoch 27/250 - Val Loss: 0.0393\n",
            "Epoch 28/250 - Val Loss: 0.0344\n",
            "Epoch 29/250 - Val Loss: 0.0332\n",
            "Epoch 30/250 - Val Loss: 0.0349\n",
            "Epoch 31/250 - Val Loss: 0.0336\n",
            "Epoch 32/250 - Val Loss: 0.0325\n",
            "Epoch 33/250 - Val Loss: 0.0336\n",
            "Epoch 34/250 - Val Loss: 0.0337\n",
            "Epoch 35/250 - Val Loss: 0.0344\n",
            "Epoch 36/250 - Val Loss: 0.0341\n",
            "Epoch 37/250 - Val Loss: 0.0323\n",
            "Epoch 38/250 - Val Loss: 0.0313\n",
            "Epoch 39/250 - Val Loss: 0.0345\n",
            "Epoch 40/250 - Val Loss: 0.0364\n",
            "Epoch 41/250 - Val Loss: 0.0321\n",
            "Epoch 42/250 - Val Loss: 0.0343\n",
            "Epoch 43/250 - Val Loss: 0.0331\n",
            "Epoch 44/250 - Val Loss: 0.0322\n",
            "Epoch 45/250 - Val Loss: 0.0336\n",
            "Epoch 46/250 - Val Loss: 0.0349\n",
            "Epoch 47/250 - Val Loss: 0.0336\n",
            "Epoch 48/250 - Val Loss: 0.0317\n",
            "Epoch 49/250 - Val Loss: 0.0307\n",
            "Epoch 50/250 - Val Loss: 0.0319\n",
            "Epoch 51/250 - Val Loss: 0.0298\n",
            "Epoch 52/250 - Val Loss: 0.0300\n",
            "Epoch 53/250 - Val Loss: 0.0313\n",
            "Epoch 54/250 - Val Loss: 0.0318\n",
            "Epoch 55/250 - Val Loss: 0.0283\n",
            "Epoch 56/250 - Val Loss: 0.0283\n",
            "Epoch 57/250 - Val Loss: 0.0409\n",
            "Epoch 58/250 - Val Loss: 0.0311\n",
            "Epoch 59/250 - Val Loss: 0.0297\n",
            "Epoch 60/250 - Val Loss: 0.0260\n",
            "Epoch 61/250 - Val Loss: 0.0284\n",
            "Epoch 62/250 - Val Loss: 0.0284\n",
            "Epoch 63/250 - Val Loss: 0.0281\n",
            "Epoch 64/250 - Val Loss: 0.0271\n",
            "Epoch 65/250 - Val Loss: 0.0265\n",
            "Epoch 66/250 - Val Loss: 0.0284\n",
            "Epoch 67/250 - Val Loss: 0.0283\n",
            "Epoch 68/250 - Val Loss: 0.0265\n",
            "Epoch 69/250 - Val Loss: 0.0254\n",
            "Epoch 70/250 - Val Loss: 0.0309\n",
            "Epoch 71/250 - Val Loss: 0.0320\n",
            "Epoch 72/250 - Val Loss: 0.0243\n",
            "Epoch 73/250 - Val Loss: 0.0291\n",
            "Epoch 74/250 - Val Loss: 0.0255\n",
            "Epoch 75/250 - Val Loss: 0.0236\n",
            "Epoch 76/250 - Val Loss: 0.0268\n",
            "Epoch 77/250 - Val Loss: 0.0294\n",
            "Epoch 78/250 - Val Loss: 0.0297\n",
            "Epoch 79/250 - Val Loss: 0.0242\n",
            "Epoch 80/250 - Val Loss: 0.0225\n",
            "Epoch 81/250 - Val Loss: 0.0242\n",
            "Epoch 82/250 - Val Loss: 0.0234\n",
            "Epoch 83/250 - Val Loss: 0.0194\n",
            "Epoch 84/250 - Val Loss: 0.0215\n",
            "Epoch 85/250 - Val Loss: 0.0232\n",
            "Epoch 86/250 - Val Loss: 0.0212\n",
            "Epoch 87/250 - Val Loss: 0.0203\n",
            "Epoch 88/250 - Val Loss: 0.0224\n",
            "Epoch 89/250 - Val Loss: 0.0184\n",
            "Epoch 90/250 - Val Loss: 0.0224\n",
            "Epoch 91/250 - Val Loss: 0.0226\n",
            "Epoch 92/250 - Val Loss: 0.0235\n",
            "Epoch 93/250 - Val Loss: 0.0219\n",
            "Epoch 94/250 - Val Loss: 0.0192\n",
            "Epoch 95/250 - Val Loss: 0.0212\n",
            "Epoch 96/250 - Val Loss: 0.0185\n",
            "Epoch 97/250 - Val Loss: 0.0177\n",
            "Epoch 98/250 - Val Loss: 0.0286\n",
            "Epoch 99/250 - Val Loss: 0.0232\n",
            "Epoch 100/250 - Val Loss: 0.0188\n",
            "Epoch 101/250 - Val Loss: 0.0173\n",
            "Epoch 102/250 - Val Loss: 0.0169\n",
            "Epoch 103/250 - Val Loss: 0.0210\n",
            "Epoch 104/250 - Val Loss: 0.0158\n",
            "Epoch 105/250 - Val Loss: 0.0143\n",
            "Epoch 106/250 - Val Loss: 0.0161\n",
            "Epoch 107/250 - Val Loss: 0.0166\n",
            "Epoch 108/250 - Val Loss: 0.0152\n",
            "Epoch 109/250 - Val Loss: 0.0169\n",
            "Epoch 110/250 - Val Loss: 0.0162\n",
            "Epoch 111/250 - Val Loss: 0.0165\n",
            "Epoch 112/250 - Val Loss: 0.0138\n",
            "Epoch 113/250 - Val Loss: 0.0192\n",
            "Epoch 114/250 - Val Loss: 0.0165\n",
            "Epoch 115/250 - Val Loss: 0.0197\n",
            "Epoch 116/250 - Val Loss: 0.0151\n",
            "Epoch 117/250 - Val Loss: 0.0128\n",
            "Epoch 118/250 - Val Loss: 0.0175\n",
            "Epoch 119/250 - Val Loss: 0.0135\n",
            "Epoch 120/250 - Val Loss: 0.0143\n",
            "Epoch 121/250 - Val Loss: 0.0128\n",
            "Epoch 122/250 - Val Loss: 0.0157\n",
            "Epoch 123/250 - Val Loss: 0.0154\n",
            "Epoch 124/250 - Val Loss: 0.0119\n",
            "Epoch 125/250 - Val Loss: 0.0150\n",
            "Epoch 126/250 - Val Loss: 0.0169\n",
            "Epoch 127/250 - Val Loss: 0.0169\n",
            "Epoch 128/250 - Val Loss: 0.0123\n",
            "Epoch 129/250 - Val Loss: 0.0138\n",
            "Epoch 130/250 - Val Loss: 0.0124\n",
            "Epoch 131/250 - Val Loss: 0.0120\n",
            "Epoch 132/250 - Val Loss: 0.0161\n",
            "Epoch 133/250 - Val Loss: 0.0115\n",
            "Epoch 134/250 - Val Loss: 0.0135\n",
            "Epoch 135/250 - Val Loss: 0.0145\n",
            "Epoch 136/250 - Val Loss: 0.0175\n",
            "Epoch 137/250 - Val Loss: 0.0137\n",
            "Epoch 138/250 - Val Loss: 0.0115\n",
            "Epoch 139/250 - Val Loss: 0.0143\n",
            "Epoch 140/250 - Val Loss: 0.0129\n",
            "Epoch 141/250 - Val Loss: 0.0144\n",
            "Epoch 142/250 - Val Loss: 0.0110\n",
            "Epoch 143/250 - Val Loss: 0.0136\n",
            "Epoch 144/250 - Val Loss: 0.0175\n",
            "Epoch 145/250 - Val Loss: 0.0116\n",
            "Epoch 146/250 - Val Loss: 0.0100\n",
            "Epoch 147/250 - Val Loss: 0.0110\n",
            "Epoch 148/250 - Val Loss: 0.0119\n",
            "Epoch 149/250 - Val Loss: 0.0096\n",
            "Epoch 150/250 - Val Loss: 0.0121\n",
            "Epoch 151/250 - Val Loss: 0.0127\n",
            "Epoch 152/250 - Val Loss: 0.0125\n",
            "Epoch 153/250 - Val Loss: 0.0140\n",
            "Epoch 154/250 - Val Loss: 0.0115\n",
            "Epoch 155/250 - Val Loss: 0.0129\n",
            "Epoch 156/250 - Val Loss: 0.0126\n",
            "Epoch 157/250 - Val Loss: 0.0100\n",
            "Epoch 158/250 - Val Loss: 0.0135\n",
            "Epoch 159/250 - Val Loss: 0.0126\n",
            "Epoch 160/250 - Val Loss: 0.0165\n",
            "Epoch 161/250 - Val Loss: 0.0105\n",
            "Epoch 162/250 - Val Loss: 0.0109\n",
            "Epoch 163/250 - Val Loss: 0.0097\n",
            "Epoch 164/250 - Val Loss: 0.0107\n",
            "Epoch 165/250 - Val Loss: 0.0116\n",
            "Epoch 166/250 - Val Loss: 0.0109\n",
            "Epoch 167/250 - Val Loss: 0.0101\n",
            "Epoch 168/250 - Val Loss: 0.0128\n",
            "Epoch 169/250 - Val Loss: 0.0111\n",
            "Epoch 170/250 - Val Loss: 0.0107\n",
            "Epoch 171/250 - Val Loss: 0.0114\n",
            "Epoch 172/250 - Val Loss: 0.0100\n",
            "Epoch 173/250 - Val Loss: 0.0118\n",
            "Epoch 174/250 - Val Loss: 0.0096\n",
            "Epoch 175/250 - Val Loss: 0.0105\n",
            "Epoch 176/250 - Val Loss: 0.0103\n",
            "Epoch 177/250 - Val Loss: 0.0123\n",
            "Epoch 178/250 - Val Loss: 0.0101\n",
            "Epoch 179/250 - Val Loss: 0.0102\n",
            "Epoch 180/250 - Val Loss: 0.0139\n",
            "Epoch 181/250 - Val Loss: 0.0118\n",
            "Epoch 182/250 - Val Loss: 0.0117\n",
            "Epoch 183/250 - Val Loss: 0.0106\n",
            "Epoch 184/250 - Val Loss: 0.0114\n",
            "Epoch 185/250 - Val Loss: 0.0113\n",
            "Epoch 186/250 - Val Loss: 0.0203\n",
            "Epoch 187/250 - Val Loss: 0.0120\n",
            "Epoch 188/250 - Val Loss: 0.0106\n",
            "Epoch 189/250 - Val Loss: 0.0133\n",
            "Epoch 190/250 - Val Loss: 0.0145\n",
            "Epoch 191/250 - Val Loss: 0.0101\n",
            "Epoch 192/250 - Val Loss: 0.0096\n",
            "Epoch 193/250 - Val Loss: 0.0091\n",
            "Epoch 194/250 - Val Loss: 0.0123\n",
            "Epoch 195/250 - Val Loss: 0.0097\n",
            "Epoch 196/250 - Val Loss: 0.0106\n",
            "Epoch 197/250 - Val Loss: 0.0090\n",
            "Epoch 198/250 - Val Loss: 0.0099\n",
            "Epoch 199/250 - Val Loss: 0.0088\n",
            "Epoch 200/250 - Val Loss: 0.0146\n",
            "Epoch 201/250 - Val Loss: 0.0099\n",
            "Epoch 202/250 - Val Loss: 0.0092\n",
            "Epoch 203/250 - Val Loss: 0.0105\n",
            "Epoch 204/250 - Val Loss: 0.0104\n",
            "Epoch 205/250 - Val Loss: 0.0115\n",
            "Epoch 206/250 - Val Loss: 0.0101\n",
            "Epoch 207/250 - Val Loss: 0.0102\n",
            "Epoch 208/250 - Val Loss: 0.0097\n",
            "Epoch 209/250 - Val Loss: 0.0107\n",
            "Epoch 210/250 - Val Loss: 0.0093\n",
            "Epoch 211/250 - Val Loss: 0.0108\n",
            "Epoch 212/250 - Val Loss: 0.0092\n",
            "Epoch 213/250 - Val Loss: 0.0128\n",
            "Epoch 214/250 - Val Loss: 0.0088\n",
            "Epoch 215/250 - Val Loss: 0.0093\n",
            "Epoch 216/250 - Val Loss: 0.0133\n",
            "Epoch 217/250 - Val Loss: 0.0082\n",
            "Epoch 218/250 - Val Loss: 0.0157\n",
            "Epoch 219/250 - Val Loss: 0.0167\n",
            "Epoch 220/250 - Val Loss: 0.0095\n",
            "Epoch 221/250 - Val Loss: 0.0101\n",
            "Epoch 222/250 - Val Loss: 0.0087\n",
            "Epoch 223/250 - Val Loss: 0.0107\n",
            "Epoch 224/250 - Val Loss: 0.0116\n",
            "Epoch 225/250 - Val Loss: 0.0094\n",
            "Epoch 226/250 - Val Loss: 0.0087\n",
            "Epoch 227/250 - Val Loss: 0.0114\n",
            "Epoch 228/250 - Val Loss: 0.0108\n",
            "Epoch 229/250 - Val Loss: 0.0095\n",
            "Epoch 230/250 - Val Loss: 0.0091\n",
            "Epoch 231/250 - Val Loss: 0.0094\n",
            "Epoch 232/250 - Val Loss: 0.0113\n",
            "Epoch 233/250 - Val Loss: 0.0091\n",
            "Epoch 234/250 - Val Loss: 0.0100\n",
            "Epoch 235/250 - Val Loss: 0.0114\n",
            "Epoch 236/250 - Val Loss: 0.0108\n",
            "Epoch 237/250 - Val Loss: 0.0100\n",
            "Epoch 238/250 - Val Loss: 0.0087\n",
            "Epoch 239/250 - Val Loss: 0.0102\n",
            "Epoch 240/250 - Val Loss: 0.0107\n",
            "Epoch 241/250 - Val Loss: 0.0095\n",
            "Epoch 242/250 - Val Loss: 0.0092\n",
            "Epoch 243/250 - Val Loss: 0.0094\n",
            "Epoch 244/250 - Val Loss: 0.0088\n",
            "Epoch 245/250 - Val Loss: 0.0093\n",
            "Epoch 246/250 - Val Loss: 0.0090\n",
            "Epoch 247/250 - Val Loss: 0.0091\n",
            "Epoch 248/250 - Val Loss: 0.0106\n",
            "Epoch 249/250 - Val Loss: 0.0093\n",
            "Epoch 250/250 - Val Loss: 0.0147\n",
            "\n",
            "---- Fold 4/5 ----\n",
            "Epoch 1/250 - Val Loss: 0.0661\n",
            "Epoch 2/250 - Val Loss: 0.0642\n",
            "Epoch 3/250 - Val Loss: 0.0539\n",
            "Epoch 4/250 - Val Loss: 0.0518\n",
            "Epoch 5/250 - Val Loss: 0.0464\n",
            "Epoch 6/250 - Val Loss: 0.0440\n",
            "Epoch 7/250 - Val Loss: 0.0433\n",
            "Epoch 8/250 - Val Loss: 0.0683\n",
            "Epoch 9/250 - Val Loss: 0.0412\n",
            "Epoch 10/250 - Val Loss: 0.0401\n",
            "Epoch 11/250 - Val Loss: 0.0381\n",
            "Epoch 12/250 - Val Loss: 0.0380\n",
            "Epoch 13/250 - Val Loss: 0.0405\n",
            "Epoch 14/250 - Val Loss: 0.0388\n",
            "Epoch 15/250 - Val Loss: 0.0370\n",
            "Epoch 16/250 - Val Loss: 0.0348\n",
            "Epoch 17/250 - Val Loss: 0.0360\n",
            "Epoch 18/250 - Val Loss: 0.0353\n",
            "Epoch 19/250 - Val Loss: 0.0369\n",
            "Epoch 20/250 - Val Loss: 0.0360\n",
            "Epoch 21/250 - Val Loss: 0.0358\n",
            "Epoch 22/250 - Val Loss: 0.0335\n",
            "Epoch 23/250 - Val Loss: 0.0353\n",
            "Epoch 24/250 - Val Loss: 0.0361\n",
            "Epoch 25/250 - Val Loss: 0.0354\n",
            "Epoch 26/250 - Val Loss: 0.0358\n",
            "Epoch 27/250 - Val Loss: 0.0331\n",
            "Epoch 28/250 - Val Loss: 0.0320\n",
            "Epoch 29/250 - Val Loss: 0.0318\n",
            "Epoch 30/250 - Val Loss: 0.0315\n",
            "Epoch 31/250 - Val Loss: 0.0342\n",
            "Epoch 32/250 - Val Loss: 0.0367\n",
            "Epoch 33/250 - Val Loss: 0.0329\n",
            "Epoch 34/250 - Val Loss: 0.0308\n",
            "Epoch 35/250 - Val Loss: 0.0331\n",
            "Epoch 36/250 - Val Loss: 0.0301\n",
            "Epoch 37/250 - Val Loss: 0.0312\n",
            "Epoch 38/250 - Val Loss: 0.0339\n",
            "Epoch 39/250 - Val Loss: 0.0308\n",
            "Epoch 40/250 - Val Loss: 0.0310\n",
            "Epoch 41/250 - Val Loss: 0.0304\n",
            "Epoch 42/250 - Val Loss: 0.0301\n",
            "Epoch 43/250 - Val Loss: 0.0306\n",
            "Epoch 44/250 - Val Loss: 0.0295\n",
            "Epoch 45/250 - Val Loss: 0.0311\n",
            "Epoch 46/250 - Val Loss: 0.0302\n",
            "Epoch 47/250 - Val Loss: 0.0328\n",
            "Epoch 48/250 - Val Loss: 0.0293\n",
            "Epoch 49/250 - Val Loss: 0.0311\n",
            "Epoch 50/250 - Val Loss: 0.0298\n",
            "Epoch 51/250 - Val Loss: 0.0307\n",
            "Epoch 52/250 - Val Loss: 0.0285\n",
            "Epoch 53/250 - Val Loss: 0.0289\n",
            "Epoch 54/250 - Val Loss: 0.0303\n",
            "Epoch 55/250 - Val Loss: 0.0281\n",
            "Epoch 56/250 - Val Loss: 0.0283\n",
            "Epoch 57/250 - Val Loss: 0.0277\n",
            "Epoch 58/250 - Val Loss: 0.0278\n",
            "Epoch 59/250 - Val Loss: 0.0271\n",
            "Epoch 60/250 - Val Loss: 0.0277\n",
            "Epoch 61/250 - Val Loss: 0.0274\n",
            "Epoch 62/250 - Val Loss: 0.0257\n",
            "Epoch 63/250 - Val Loss: 0.0266\n",
            "Epoch 64/250 - Val Loss: 0.0306\n",
            "Epoch 65/250 - Val Loss: 0.0263\n",
            "Epoch 66/250 - Val Loss: 0.0265\n",
            "Epoch 67/250 - Val Loss: 0.0254\n",
            "Epoch 68/250 - Val Loss: 0.0257\n",
            "Epoch 69/250 - Val Loss: 0.0285\n",
            "Epoch 70/250 - Val Loss: 0.0239\n",
            "Epoch 71/250 - Val Loss: 0.0241\n",
            "Epoch 72/250 - Val Loss: 0.0241\n",
            "Epoch 73/250 - Val Loss: 0.0236\n",
            "Epoch 74/250 - Val Loss: 0.0256\n",
            "Epoch 75/250 - Val Loss: 0.0311\n",
            "Epoch 76/250 - Val Loss: 0.0256\n",
            "Epoch 77/250 - Val Loss: 0.0231\n",
            "Epoch 78/250 - Val Loss: 0.0264\n",
            "Epoch 79/250 - Val Loss: 0.0226\n",
            "Epoch 80/250 - Val Loss: 0.0249\n",
            "Epoch 81/250 - Val Loss: 0.0216\n",
            "Epoch 82/250 - Val Loss: 0.0252\n",
            "Epoch 83/250 - Val Loss: 0.0260\n",
            "Epoch 84/250 - Val Loss: 0.0231\n",
            "Epoch 85/250 - Val Loss: 0.0232\n",
            "Epoch 86/250 - Val Loss: 0.0206\n",
            "Epoch 87/250 - Val Loss: 0.0247\n",
            "Epoch 88/250 - Val Loss: 0.0202\n",
            "Epoch 89/250 - Val Loss: 0.0209\n",
            "Epoch 90/250 - Val Loss: 0.0194\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[31], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mCross_validation\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcv\u001b[39;00m\n\u001b[0;32m      8\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(cv)\n\u001b[1;32m---> 10\u001b[0m train_loss, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_kfold_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSpectraCNN\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Masters_Data\\Raman_spetr\\git_testing\\Raman_CNN\\Cross_validation.py:58\u001b[0m, in \u001b[0;36mrun_kfold_cv\u001b[1;34m(spectra, targets, k, batch_size, epochs, model_class)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m val_loader: \n\u001b[1;32m---> 58\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m         epoch_val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m criterion(out, y)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     61\u001b[0m avg_val_loss \u001b[38;5;241m=\u001b[39m epoch_val_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(val_loader)\n",
            "File \u001b[1;32mc:\\Users\\joaof\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\joaof\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32md:\\Masters_Data\\Raman_spetr\\git_testing\\Raman_CNN\\SpectraCNN.py:38\u001b[0m, in \u001b[0;36mSpectraCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[0;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)))\n\u001b[1;32m---> 38\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn4(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Pooling\u001b[39;00m\n\u001b[0;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_avg_pool(x)   \u001b[38;5;66;03m# (batch, 256, 1)\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\joaof\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\joaof\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\joaof\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\joaof\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from sklearn.model_selection import KFold\n",
        "from SpectraCNN import SpectraCNN\n",
        "\n",
        "\n",
        "import Cross_validation as cv\n",
        "importlib.reload(cv)\n",
        "\n",
        "train_loss, val_loss = cv.run_kfold_cv(train_data, labels_scaled, k=5, batch_size=32, epochs=250, model_class = SpectraCNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 32, 3)\n",
            "(2, 32, 3)\n",
            "(3, 32, 3)\n",
            "(4, 32, 3)\n",
            "(5, 32, 3)\n",
            "(6, 32, 3)\n",
            "torch.Size([192, 3])\n"
          ]
        }
      ],
      "source": [
        "# Load models\n",
        "models = []\n",
        "for i in range(1, 6):\n",
        "    model = SpectraCNN()\n",
        "    model.load_state_dict(torch.load(f\"C:\\\\Users\\\\joaof\\\\Downloads\\\\model{i}.pth\", map_location=\"cpu\"))\n",
        "    model.eval()\n",
        "    models.append(model)\n",
        "\n",
        "clean_test=np.array(clean_test,dtype=np.float32)\n",
        "test_loader = DataLoader(RamanDataset(clean_test,None, train=False), batch_size=32, shuffle=False)\n",
        "\n",
        "# Inference with ensemble averaging\n",
        "all_outputs = []\n",
        "with torch.no_grad():\n",
        "    for (x) in test_loader:\n",
        "\n",
        "        preds = [m(x) for m in models]                 # list of [batch_size, output_dim]\n",
        "        \n",
        "        mean_pred = torch.stack(preds).mean(dim=0)     # average over models\n",
        "        \n",
        "        all_outputs.append(mean_pred)\n",
        "        \n",
        "final_results = torch.cat(all_outputs)  # [num_samples, output_dim]\n",
        "\n",
        "final_results_descaled=Da.reversible_scaler(final_results,max=labels_max,reverse=True)\n",
        "\n",
        "#load predictions to sample_submission.csv\n",
        "\n",
        "template= pd.read_csv(\"test/sample_submission.csv\")\n",
        "\n",
        "\n",
        "averaged = final_results_descaled.reshape(-1, 2, 3).mean(axis=1)\n",
        "\n",
        "template[['Glucose', 'Sodium Acetate', 'Magnesium Sulfate']] = averaged\n",
        "\n",
        "# Save to new CSV\n",
        "template.to_csv(\"test/sample_submission.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
